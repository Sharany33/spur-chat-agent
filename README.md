# Spur â€“ Mini AI Live Chat Agent

This repository implements the take-home assignment for a Founding Full-Stack Engineer at Spur.

It is a small, end-to-end web application that simulates a customer support live chat widget, where an AI agent answers common e-commerce support questions using a real LLM API.

The focus of this project is correctness, clarity, and realistic product behavior, rather than feature breadth.

## ğŸ› ï¸ Tech Stack

- **Backend**: Node.js, TypeScript, Express
- **Database**: SQLite (via better-sqlite3)
- **Frontend**: React + Vite
- **LLM**: OpenAI Chat Completions API
- **Styling**: Custom CSS (no UI framework)

## ğŸŒ Live Demo

- **Frontend (Chat UI)**: https://spur-chat-n0mct4ovq-sharanya-bhat-ns-projects.vercel.app
- **Backend API**: https://spur-chat-backend-1g5s.onrender.com

> Note: The backend may take a few seconds to respond on first request due to cold start on Render.

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18+
- npm or yarn
- An OpenAI API key (e.g., for gpt-4o-mini)

### Backend Setup
1. **Install dependencies**
   ```bash
   cd backend
   npm install
   ```

2. **Configure environment**
   Create `.env` file in the `backend` directory:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   PORT=4000
   ```

3. **Initialize database**
   ```bash
   npm run migrate
   ```

4. **Start the server**
   ```bash
   npm run dev
   ```
   > API available at: http://localhost:4000

### Frontend Setup
1. **Install dependencies**
   ```bash
   cd ../frontend
   npm install
   ```

2. **Start development server**
   ```bash
   npm run dev
   ```
   > App available at: http://localhost:5173

## ğŸ“‹ Try It Out
Once running, try asking:
- "What is your return policy?"
- "Do you ship to USA?"
- "How long do refunds take?"

## ğŸ”Œ API Reference

### `POST /chat/message`
Send a user message and get an AI response.

**Request**
```json
{
  "message": "What is your return policy?",
  "sessionId": "optional-session-id"
}
```

**Response**
```json
{
  "reply": "Our return policy allows returns within 30 days...",
  "sessionId": "generated-or-existing-session-id"
}
```

### `GET /chat/history/:sessionId`
Get chat history for a session.

**Response**
```json
{
  "sessionId": "abc-123",
  "createdAt": 1735532610000,
  "messages": [
    {
      "sender": "user",
      "text": "Do you ship to USA?",
      "created_at": 1735532610000
    },
    {
      "sender": "ai",
      "text": "Yes, we ship internationally...",
      "created_at": 1735532611000
    }
  ]
}
```

### `GET /health`
Health check endpoint.

**Response**
```json
{ "status": "ok" }
```

## ğŸ—„ï¸ Data Model

### Tables
**`conversations`**
- `id` (TEXT, primary key â€” sessionId)
- `created_at` (INTEGER, timestamp in ms)

**`messages`**
- `id` (INTEGER, primary key, autoincrement)
- `conversation_id` (TEXT, foreign key)
- `sender` ("user" or "ai")
- `text` (TEXT)
- `created_at` (INTEGER, timestamp in ms)

## ğŸ¤– LLM Integration
- **File**: `backend/src/llm.ts`
- **Main Function**: `generateReply(history, userMessage): Promise<string>`

### Features
- Contextual responses using conversation history
- Built-in error handling and fallback responses
- Token limit and temperature controls
- Fixed system prompt defining the AI as a support agent
- Includes FAQ knowledge (shipping, returns, refunds, support hours)
- Returns friendly fallback messages when LLM is unavailable

## ğŸ–¥ï¸ Frontend
- **Location**: `frontend/src/App.tsx`
- **State Management**: Messages, session, input state
- **Features**:
  - Persistent chat history
  - Typing indicators
  - Error handling
  - Mobile-responsive design

### Session & Conversation Flow
- A `sessionId` is generated by the backend on the first message
- The frontend stores this `sessionId` in `localStorage`
- On page reload, the frontend calls `GET /chat/history/:sessionId`
- If the session is invalid or missing, a new chat is started automatically

## ğŸ—ï¸ Architecture
1. **API Layer**: Express.js routes
2. **LLM Service**: OpenAI integration
3. **Data Layer**: SQLite database
4. **Frontend**: React application

This separation makes it easy to add new channels (WhatsApp, Instagram, etc.) by reusing the same LLM and data layers with different transport adapters.

## ğŸ”’ Security
- Environment-based configuration
- Input validation
- Error handling

## ğŸš€ Future Improvements
- [ ] Token streaming
- [ ] Pagination for message history
- [ ] Rate limiting
- [ ] WebSockets for real-time updates
- [ ] Multi-channel support SDK

## ğŸ“ Design Decisions
This project intentionally focuses on a single, realistic support chat experience rather than a full dashboard.

Key decisions:
- Chat is the primary focus, reflecting real support workflows
- Copy and tone are aligned with Spurâ€™s product messaging but adapted for an operator-facing context
- UX details like typing indicators, disabled send states, and auto-scroll were added for realism
- The UI is calm and functional, reflecting Spurâ€™s â€œboring but makes moneyâ€ philosophy
- The goal was to build something believable and extensible, not flashy.

## ğŸ•’ï¸ If I Had More Timeâ€¦
- Add pagination or â€œload earlier messagesâ€ for long conversations
- Stream LLM responses token-by-token
- Add rate limiting and abuse protection
- Introduce WebSockets or SSE for more real-time behavior
- Extract a small SDK so other channels can reuse the same backend cleanly